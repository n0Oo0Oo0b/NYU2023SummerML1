{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "colab": {
   "name": "demo_tf_keras_basics.ipynb",
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "29KIsL99-rNa",
    "ExecuteTime": {
     "end_time": "2023-06-27T16:33:42.942981Z",
     "start_time": "2023-06-27T16:33:42.907526Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNky-aEF-rNb",
    "ExecuteTime": {
     "end_time": "2023-06-27T14:55:29.326857Z",
     "start_time": "2023-06-27T14:55:29.321075Z"
    }
   },
   "source": [
    "Keras: https://keras.io/"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18-HEKzy-rNb"
   },
   "source": [
    "### Define the neural network model\n",
    "\n",
    "\n",
    "Using the Keras Sequential API we define a model with three fully connected layers. \n",
    "- The first layer has an input with size of $[\\texttt{batch_size}, \\texttt{d0}, \\texttt{d1}]$. However, in the input shape argument we only need to specify $[\\texttt{d0}, \\texttt{d1}]$, if the input does not have a second dimension $\\texttt{d1}$ then in the input shape argument we can write $(\\texttt{d0}, )$.\n",
    "- The first layer also has an activation function, which we can specify using the activation argument, common choices are ReLU, sigmoid, tanh and linear. \n",
    "- By simply adding another Dense object layer to the Sequential model we can build the second layer.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*ZB6H4HuF58VcMOWbdpcRxQ.png\" alt=\"NN_Image\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqv4jK8d-rNb"
   },
   "source": [
    "$$h_1 = g(a_1) = g(W_1^Tx + b_1)$$\n",
    "\n",
    "$$h_2 = g(a_2) = g(W_2^Th_1 + b_2)$$\n",
    "\n",
    "$$y = g(a_3) = g(W_3^Th_2 + b_3)$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NF-idJAV-rNc",
    "ExecuteTime": {
     "end_time": "2023-06-27T16:33:42.967566Z",
     "start_time": "2023-06-27T16:33:42.922547Z"
    }
   },
   "source": [
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(1,)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bulc3pym-rNc",
    "outputId": "42c8d987-16f1-4c99-eeaa-2acc5714b7ca",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-06-27T16:33:42.968520Z",
     "start_time": "2023-06-27T16:33:42.943239Z"
    }
   },
   "source": [
    "model.summary()"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 16)                32        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 321 (1.25 KB)\n",
      "Trainable params: 321 (1.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "48vcyKTs-rNd",
    "ExecuteTime": {
     "end_time": "2023-06-27T16:33:42.968699Z",
     "start_time": "2023-06-27T16:33:42.956985Z"
    }
   },
   "source": [
    "model.compile(\n",
    "    optimizer='adam', # 'rmsprop', 'sgd'\n",
    "    loss='mean_squared_error', # 'mae'\n",
    "    metrics='mse'\n",
    ")"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##ReLU\n",
    "<img src=\"https://miro.medium.com/max/1400/1*XxxiA0jJvPrHEJHD4z893g.png\" alt=\"NN_Image\" width=\"800\">"
   ],
   "metadata": {
    "id": "G1mI3-QwDNYi"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tY1znnoZ-rNd"
   },
   "source": [
    "### Generate random data\n",
    "\n",
    "In this example we will try to fit the curve\n",
    "$$f(x) = x\\cos(x) + \\sin^2(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NplHZnCU-rNd",
    "ExecuteTime": {
     "end_time": "2023-06-27T16:33:42.969125Z",
     "start_time": "2023-06-27T16:33:42.961979Z"
    }
   },
   "source": [
    "X_train = tf.random.uniform(shape=[1000, ], minval=0, maxval=12)\n",
    "Y_train = X_train * tf.cos(X_train) + tf.sin(X_train) ** 2 + 0.5*tf.random.normal(shape=[1000, ])\n",
    "\n",
    "X_test = tf.random.uniform(shape=[500, ], minval=0, maxval=12)\n",
    "Y_test = X_test * tf.cos(X_test) + tf.sin(X_test) ** 2 + 0.5*tf.random.normal(shape=[500, ])"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BLLMQ0J6-rNe",
    "outputId": "96223693-6de7-4e33-f356-765f96cc2b01",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-06-27T16:33:42.976276Z",
     "start_time": "2023-06-27T16:33:42.969240Z"
    }
   },
   "source": [
    "X_train = tf.reshape(X_train, (-1, 1))\n",
    "Y_train = tf.reshape(Y_train, (-1, 1))\n",
    "X_train.shape, Y_train.shape"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "(TensorShape([1000, 1]), TensorShape([1000, 1]))"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgPCgXXr-rNe"
   },
   "source": [
    "Now we can visualize the data, note that since the data is not sorted we should use a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jpTabyge-rNe",
    "outputId": "f9dce3e3-a278-4749-e419-bbe2e2bc9bc1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "ExecuteTime": {
     "end_time": "2023-06-27T16:33:43.057895Z",
     "start_time": "2023-06-27T16:33:42.977009Z"
    }
   },
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_train.numpy(), Y_train.numpy())\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 800x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3Gza5OY-rNe"
   },
   "source": [
    "### Training\n",
    "\n",
    "Using the generated data we can train the neural network model, we train it for 500 epochs (one epoch is one pass over the entire dataset) with a batch size of 64."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "xBzbND2l-rNf",
    "outputId": "64f68146-2ef0-451d-cb9f-eb6e72795f84",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-06-27T16:33:52.962828Z",
     "start_time": "2023-06-27T16:33:43.059586Z"
    }
   },
   "source": [
    "# training_history = model.fit(features, labels, epochs=epochs, batch_size=batch_size)\n",
    "history = model.fit(X_train, Y_train, epochs=1000, validation_split=0.1, batch_size=32)"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 22.3166 - mse: 22.3166 - val_loss: 20.9466 - val_mse: 20.9466\n",
      "Epoch 2/1000\n",
      "29/29 [==============================] - 0s 791us/step - loss: 21.3563 - mse: 21.3563 - val_loss: 20.4570 - val_mse: 20.4570\n",
      "Epoch 3/1000\n",
      "29/29 [==============================] - 0s 785us/step - loss: 21.2826 - mse: 21.2826 - val_loss: 20.3471 - val_mse: 20.3471\n",
      "Epoch 4/1000\n",
      "29/29 [==============================] - 0s 803us/step - loss: 21.2588 - mse: 21.2588 - val_loss: 20.4271 - val_mse: 20.4271\n",
      "Epoch 5/1000\n",
      "29/29 [==============================] - 0s 769us/step - loss: 21.2476 - mse: 21.2476 - val_loss: 20.3415 - val_mse: 20.3415\n",
      "Epoch 6/1000\n",
      "29/29 [==============================] - 0s 802us/step - loss: 21.2478 - mse: 21.2478 - val_loss: 20.2889 - val_mse: 20.2889\n",
      "Epoch 7/1000\n",
      "29/29 [==============================] - 0s 741us/step - loss: 21.2595 - mse: 21.2595 - val_loss: 20.1696 - val_mse: 20.1696\n",
      "Epoch 8/1000\n",
      "29/29 [==============================] - 0s 751us/step - loss: 21.1942 - mse: 21.1942 - val_loss: 20.2680 - val_mse: 20.2680\n",
      "Epoch 9/1000\n",
      "29/29 [==============================] - 0s 704us/step - loss: 21.1763 - mse: 21.1763 - val_loss: 20.2669 - val_mse: 20.2669\n",
      "Epoch 10/1000\n",
      "29/29 [==============================] - 0s 745us/step - loss: 21.2200 - mse: 21.2200 - val_loss: 20.1589 - val_mse: 20.1589\n",
      "Epoch 11/1000\n",
      "29/29 [==============================] - 0s 726us/step - loss: 21.1576 - mse: 21.1576 - val_loss: 20.2649 - val_mse: 20.2649\n",
      "Epoch 12/1000\n",
      "29/29 [==============================] - 0s 702us/step - loss: 21.1439 - mse: 21.1439 - val_loss: 20.1545 - val_mse: 20.1545\n",
      "Epoch 13/1000\n",
      "29/29 [==============================] - 0s 752us/step - loss: 21.1264 - mse: 21.1264 - val_loss: 20.1944 - val_mse: 20.1944\n",
      "Epoch 14/1000\n",
      "29/29 [==============================] - 0s 760us/step - loss: 21.1201 - mse: 21.1201 - val_loss: 20.1789 - val_mse: 20.1789\n",
      "Epoch 15/1000\n",
      "29/29 [==============================] - 0s 751us/step - loss: 21.1943 - mse: 21.1943 - val_loss: 20.2441 - val_mse: 20.2441\n",
      "Epoch 16/1000\n",
      "29/29 [==============================] - 0s 734us/step - loss: 21.0978 - mse: 21.0978 - val_loss: 20.1191 - val_mse: 20.1191\n",
      "Epoch 17/1000\n",
      "29/29 [==============================] - 0s 733us/step - loss: 21.0930 - mse: 21.0930 - val_loss: 20.1045 - val_mse: 20.1045\n",
      "Epoch 18/1000\n",
      "29/29 [==============================] - 0s 748us/step - loss: 21.1000 - mse: 21.1000 - val_loss: 20.1002 - val_mse: 20.1002\n",
      "Epoch 19/1000\n",
      "29/29 [==============================] - 0s 731us/step - loss: 21.1158 - mse: 21.1158 - val_loss: 20.1580 - val_mse: 20.1580\n",
      "Epoch 20/1000\n",
      "29/29 [==============================] - 0s 750us/step - loss: 21.0754 - mse: 21.0754 - val_loss: 20.1100 - val_mse: 20.1100\n",
      "Epoch 21/1000\n",
      "29/29 [==============================] - 0s 746us/step - loss: 21.0719 - mse: 21.0719 - val_loss: 20.2228 - val_mse: 20.2228\n",
      "Epoch 22/1000\n",
      "29/29 [==============================] - 0s 728us/step - loss: 21.0783 - mse: 21.0783 - val_loss: 20.2274 - val_mse: 20.2274\n",
      "Epoch 23/1000\n",
      "29/29 [==============================] - 0s 723us/step - loss: 21.0680 - mse: 21.0680 - val_loss: 20.1572 - val_mse: 20.1572\n",
      "Epoch 24/1000\n",
      "29/29 [==============================] - 0s 734us/step - loss: 21.0644 - mse: 21.0644 - val_loss: 20.0922 - val_mse: 20.0922\n",
      "Epoch 25/1000\n",
      "29/29 [==============================] - 0s 703us/step - loss: 21.0956 - mse: 21.0956 - val_loss: 20.0641 - val_mse: 20.0641\n",
      "Epoch 26/1000\n",
      "29/29 [==============================] - 0s 715us/step - loss: 21.0543 - mse: 21.0543 - val_loss: 20.1178 - val_mse: 20.1178\n",
      "Epoch 27/1000\n",
      "29/29 [==============================] - 0s 704us/step - loss: 21.0749 - mse: 21.0749 - val_loss: 20.1395 - val_mse: 20.1395\n",
      "Epoch 28/1000\n",
      "29/29 [==============================] - 0s 809us/step - loss: 21.0560 - mse: 21.0560 - val_loss: 20.1098 - val_mse: 20.1098\n",
      "Epoch 29/1000\n",
      "29/29 [==============================] - 0s 679us/step - loss: 21.0623 - mse: 21.0623 - val_loss: 20.1071 - val_mse: 20.1071\n",
      "Epoch 30/1000\n",
      "29/29 [==============================] - 0s 704us/step - loss: 21.0513 - mse: 21.0513 - val_loss: 20.0341 - val_mse: 20.0341\n",
      "Epoch 31/1000\n",
      "29/29 [==============================] - 0s 729us/step - loss: 21.0540 - mse: 21.0540 - val_loss: 20.0259 - val_mse: 20.0259\n",
      "Epoch 32/1000\n",
      "29/29 [==============================] - 0s 719us/step - loss: 21.0484 - mse: 21.0484 - val_loss: 20.0042 - val_mse: 20.0042\n",
      "Epoch 33/1000\n",
      "29/29 [==============================] - 0s 696us/step - loss: 21.0337 - mse: 21.0337 - val_loss: 20.0646 - val_mse: 20.0646\n",
      "Epoch 34/1000\n",
      "29/29 [==============================] - 0s 688us/step - loss: 21.0329 - mse: 21.0329 - val_loss: 20.0807 - val_mse: 20.0807\n",
      "Epoch 35/1000\n",
      "29/29 [==============================] - 0s 738us/step - loss: 21.0631 - mse: 21.0631 - val_loss: 20.1465 - val_mse: 20.1465\n",
      "Epoch 36/1000\n",
      "29/29 [==============================] - 0s 689us/step - loss: 21.0545 - mse: 21.0545 - val_loss: 20.0355 - val_mse: 20.0355\n",
      "Epoch 37/1000\n",
      "29/29 [==============================] - 0s 712us/step - loss: 21.0893 - mse: 21.0893 - val_loss: 20.1022 - val_mse: 20.1022\n",
      "Epoch 38/1000\n",
      "29/29 [==============================] - 0s 747us/step - loss: 21.0331 - mse: 21.0331 - val_loss: 20.0497 - val_mse: 20.0497\n",
      "Epoch 39/1000\n",
      "29/29 [==============================] - 0s 735us/step - loss: 21.0448 - mse: 21.0448 - val_loss: 20.1031 - val_mse: 20.1031\n",
      "Epoch 40/1000\n",
      "29/29 [==============================] - 0s 709us/step - loss: 21.0373 - mse: 21.0373 - val_loss: 20.1074 - val_mse: 20.1074\n",
      "Epoch 41/1000\n",
      "29/29 [==============================] - 0s 732us/step - loss: 21.0679 - mse: 21.0679 - val_loss: 19.9763 - val_mse: 19.9763\n",
      "Epoch 42/1000\n",
      "29/29 [==============================] - 0s 739us/step - loss: 21.0589 - mse: 21.0589 - val_loss: 20.2293 - val_mse: 20.2293\n",
      "Epoch 43/1000\n",
      "29/29 [==============================] - 0s 705us/step - loss: 21.0328 - mse: 21.0328 - val_loss: 20.1195 - val_mse: 20.1195\n",
      "Epoch 44/1000\n",
      "29/29 [==============================] - 0s 750us/step - loss: 21.0625 - mse: 21.0625 - val_loss: 20.1164 - val_mse: 20.1164\n",
      "Epoch 45/1000\n",
      "29/29 [==============================] - 0s 715us/step - loss: 21.0521 - mse: 21.0521 - val_loss: 19.9664 - val_mse: 19.9664\n",
      "Epoch 46/1000\n",
      "29/29 [==============================] - 0s 880us/step - loss: 21.0487 - mse: 21.0487 - val_loss: 20.0286 - val_mse: 20.0286\n",
      "Epoch 47/1000\n",
      "29/29 [==============================] - 0s 786us/step - loss: 21.0750 - mse: 21.0750 - val_loss: 20.1035 - val_mse: 20.1035\n",
      "Epoch 48/1000\n",
      "29/29 [==============================] - 0s 773us/step - loss: 21.0511 - mse: 21.0511 - val_loss: 20.2343 - val_mse: 20.2343\n",
      "Epoch 49/1000\n",
      "29/29 [==============================] - 0s 757us/step - loss: 21.0488 - mse: 21.0488 - val_loss: 20.0819 - val_mse: 20.0819\n",
      "Epoch 50/1000\n",
      "29/29 [==============================] - 0s 782us/step - loss: 21.0197 - mse: 21.0197 - val_loss: 19.9837 - val_mse: 19.9837\n",
      "Epoch 51/1000\n",
      "29/29 [==============================] - 0s 768us/step - loss: 21.0316 - mse: 21.0316 - val_loss: 20.0496 - val_mse: 20.0496\n",
      "Epoch 52/1000\n",
      "29/29 [==============================] - 0s 707us/step - loss: 21.0188 - mse: 21.0188 - val_loss: 20.0866 - val_mse: 20.0866\n",
      "Epoch 53/1000\n",
      "29/29 [==============================] - 0s 728us/step - loss: 21.0300 - mse: 21.0300 - val_loss: 20.0431 - val_mse: 20.0431\n",
      "Epoch 54/1000\n",
      "29/29 [==============================] - 0s 724us/step - loss: 21.0163 - mse: 21.0163 - val_loss: 20.1208 - val_mse: 20.1208\n",
      "Epoch 55/1000\n",
      "29/29 [==============================] - 0s 729us/step - loss: 21.0363 - mse: 21.0363 - val_loss: 20.0505 - val_mse: 20.0505\n",
      "Epoch 56/1000\n",
      "29/29 [==============================] - 0s 709us/step - loss: 21.0276 - mse: 21.0276 - val_loss: 20.0637 - val_mse: 20.0637\n",
      "Epoch 57/1000\n",
      "29/29 [==============================] - 0s 709us/step - loss: 21.0218 - mse: 21.0218 - val_loss: 20.1086 - val_mse: 20.1086\n",
      "Epoch 58/1000\n",
      "29/29 [==============================] - 0s 680us/step - loss: 21.0230 - mse: 21.0230 - val_loss: 20.0438 - val_mse: 20.0438\n",
      "Epoch 59/1000\n",
      "29/29 [==============================] - 0s 702us/step - loss: 21.0153 - mse: 21.0153 - val_loss: 19.9689 - val_mse: 19.9689\n",
      "Epoch 60/1000\n",
      "29/29 [==============================] - 0s 711us/step - loss: 21.0228 - mse: 21.0228 - val_loss: 20.0439 - val_mse: 20.0439\n",
      "Epoch 61/1000\n",
      "29/29 [==============================] - 0s 705us/step - loss: 21.0183 - mse: 21.0183 - val_loss: 20.0329 - val_mse: 20.0329\n",
      "Epoch 62/1000\n",
      "29/29 [==============================] - 0s 694us/step - loss: 21.0206 - mse: 21.0206 - val_loss: 20.0050 - val_mse: 20.0050\n",
      "Epoch 63/1000\n",
      "29/29 [==============================] - 0s 700us/step - loss: 21.0174 - mse: 21.0174 - val_loss: 20.0650 - val_mse: 20.0650\n",
      "Epoch 64/1000\n",
      "29/29 [==============================] - 0s 694us/step - loss: 21.0161 - mse: 21.0161 - val_loss: 20.0743 - val_mse: 20.0743\n",
      "Epoch 65/1000\n",
      "29/29 [==============================] - 0s 707us/step - loss: 21.0403 - mse: 21.0403 - val_loss: 20.1540 - val_mse: 20.1540\n",
      "Epoch 66/1000\n",
      "29/29 [==============================] - 0s 709us/step - loss: 21.0707 - mse: 21.0707 - val_loss: 20.1771 - val_mse: 20.1771\n",
      "Epoch 67/1000\n",
      "29/29 [==============================] - 0s 678us/step - loss: 21.0098 - mse: 21.0098 - val_loss: 19.9485 - val_mse: 19.9485\n",
      "Epoch 68/1000\n",
      "29/29 [==============================] - 0s 734us/step - loss: 21.0257 - mse: 21.0257 - val_loss: 20.0010 - val_mse: 20.0010\n",
      "Epoch 69/1000\n",
      "29/29 [==============================] - 0s 721us/step - loss: 21.0180 - mse: 21.0180 - val_loss: 20.0583 - val_mse: 20.0583\n",
      "Epoch 70/1000\n",
      "29/29 [==============================] - 0s 723us/step - loss: 21.0137 - mse: 21.0137 - val_loss: 19.9805 - val_mse: 19.9805\n",
      "Epoch 71/1000\n",
      "29/29 [==============================] - 0s 720us/step - loss: 21.0090 - mse: 21.0090 - val_loss: 20.0324 - val_mse: 20.0324\n",
      "Epoch 72/1000\n",
      "29/29 [==============================] - 0s 733us/step - loss: 21.0072 - mse: 21.0072 - val_loss: 20.0734 - val_mse: 20.0734\n",
      "Epoch 73/1000\n",
      "29/29 [==============================] - 0s 739us/step - loss: 20.9925 - mse: 20.9925 - val_loss: 20.0659 - val_mse: 20.0659\n",
      "Epoch 74/1000\n",
      "29/29 [==============================] - 0s 731us/step - loss: 20.9945 - mse: 20.9945 - val_loss: 19.9887 - val_mse: 19.9887\n",
      "Epoch 75/1000\n",
      "29/29 [==============================] - 0s 712us/step - loss: 21.0000 - mse: 21.0000 - val_loss: 20.0320 - val_mse: 20.0320\n",
      "Epoch 76/1000\n",
      "29/29 [==============================] - 0s 756us/step - loss: 21.0085 - mse: 21.0085 - val_loss: 20.0253 - val_mse: 20.0253\n",
      "Epoch 77/1000\n",
      "29/29 [==============================] - 0s 709us/step - loss: 21.0447 - mse: 21.0447 - val_loss: 20.0047 - val_mse: 20.0047\n",
      "Epoch 78/1000\n",
      "29/29 [==============================] - 0s 712us/step - loss: 21.0081 - mse: 21.0081 - val_loss: 20.1418 - val_mse: 20.1418\n",
      "Epoch 79/1000\n",
      "29/29 [==============================] - 0s 760us/step - loss: 21.0248 - mse: 21.0248 - val_loss: 20.0578 - val_mse: 20.0578\n",
      "Epoch 80/1000\n",
      "29/29 [==============================] - 0s 733us/step - loss: 21.0294 - mse: 21.0294 - val_loss: 20.1374 - val_mse: 20.1374\n",
      "Epoch 81/1000\n",
      "29/29 [==============================] - 0s 769us/step - loss: 20.9974 - mse: 20.9974 - val_loss: 19.9920 - val_mse: 19.9920\n",
      "Epoch 82/1000\n",
      "29/29 [==============================] - 0s 721us/step - loss: 20.9957 - mse: 20.9957 - val_loss: 19.9989 - val_mse: 19.9989\n",
      "Epoch 83/1000\n",
      "29/29 [==============================] - 0s 749us/step - loss: 20.9891 - mse: 20.9891 - val_loss: 19.9680 - val_mse: 19.9680\n",
      "Epoch 84/1000\n",
      "29/29 [==============================] - 0s 795us/step - loss: 20.9859 - mse: 20.9859 - val_loss: 20.0095 - val_mse: 20.0095\n",
      "Epoch 85/1000\n",
      "29/29 [==============================] - 0s 765us/step - loss: 20.9908 - mse: 20.9908 - val_loss: 19.9812 - val_mse: 19.9812\n",
      "Epoch 86/1000\n",
      "29/29 [==============================] - 0s 749us/step - loss: 20.9958 - mse: 20.9958 - val_loss: 20.0343 - val_mse: 20.0343\n",
      "Epoch 87/1000\n",
      "29/29 [==============================] - 0s 800us/step - loss: 20.9949 - mse: 20.9949 - val_loss: 19.9361 - val_mse: 19.9361\n",
      "Epoch 88/1000\n",
      "29/29 [==============================] - 0s 730us/step - loss: 21.0058 - mse: 21.0058 - val_loss: 19.9066 - val_mse: 19.9066\n",
      "Epoch 89/1000\n",
      "29/29 [==============================] - 0s 722us/step - loss: 20.9882 - mse: 20.9882 - val_loss: 19.9521 - val_mse: 19.9521\n",
      "Epoch 90/1000\n",
      "29/29 [==============================] - 0s 745us/step - loss: 20.9790 - mse: 20.9790 - val_loss: 19.9796 - val_mse: 19.9796\n",
      "Epoch 91/1000\n",
      "29/29 [==============================] - 0s 720us/step - loss: 20.9994 - mse: 20.9994 - val_loss: 19.9950 - val_mse: 19.9950\n",
      "Epoch 92/1000\n",
      "29/29 [==============================] - 0s 756us/step - loss: 21.0257 - mse: 21.0257 - val_loss: 20.1334 - val_mse: 20.1334\n",
      "Epoch 93/1000\n",
      "29/29 [==============================] - 0s 812us/step - loss: 20.9867 - mse: 20.9867 - val_loss: 20.0575 - val_mse: 20.0575\n",
      "Epoch 94/1000\n",
      "29/29 [==============================] - 0s 772us/step - loss: 20.9741 - mse: 20.9741 - val_loss: 20.0801 - val_mse: 20.0801\n",
      "Epoch 95/1000\n",
      "29/29 [==============================] - 0s 791us/step - loss: 20.9825 - mse: 20.9825 - val_loss: 20.0452 - val_mse: 20.0452\n",
      "Epoch 96/1000\n",
      "29/29 [==============================] - 0s 896us/step - loss: 20.9699 - mse: 20.9699 - val_loss: 20.0387 - val_mse: 20.0387\n",
      "Epoch 97/1000\n",
      "29/29 [==============================] - 0s 741us/step - loss: 20.9772 - mse: 20.9772 - val_loss: 19.9576 - val_mse: 19.9576\n",
      "Epoch 98/1000\n",
      "29/29 [==============================] - 0s 741us/step - loss: 21.0042 - mse: 21.0042 - val_loss: 20.1110 - val_mse: 20.1110\n",
      "Epoch 99/1000\n",
      "29/29 [==============================] - 0s 728us/step - loss: 20.9672 - mse: 20.9672 - val_loss: 19.9658 - val_mse: 19.9658\n",
      "Epoch 100/1000\n",
      "29/29 [==============================] - 0s 750us/step - loss: 20.9610 - mse: 20.9610 - val_loss: 19.9847 - val_mse: 19.9847\n",
      "Epoch 101/1000\n",
      "29/29 [==============================] - 0s 785us/step - loss: 20.9717 - mse: 20.9717 - val_loss: 20.0788 - val_mse: 20.0788\n",
      "Epoch 102/1000\n",
      "29/29 [==============================] - 0s 757us/step - loss: 20.9594 - mse: 20.9594 - val_loss: 20.0269 - val_mse: 20.0269\n",
      "Epoch 103/1000\n",
      "29/29 [==============================] - 0s 714us/step - loss: 20.9581 - mse: 20.9581 - val_loss: 20.0555 - val_mse: 20.0555\n",
      "Epoch 104/1000\n",
      "29/29 [==============================] - 0s 743us/step - loss: 20.9469 - mse: 20.9469 - val_loss: 20.0422 - val_mse: 20.0422\n",
      "Epoch 105/1000\n",
      "29/29 [==============================] - 0s 759us/step - loss: 20.9457 - mse: 20.9457 - val_loss: 20.0388 - val_mse: 20.0388\n",
      "Epoch 106/1000\n",
      "29/29 [==============================] - 0s 761us/step - loss: 20.9454 - mse: 20.9454 - val_loss: 20.0320 - val_mse: 20.0320\n",
      "Epoch 107/1000\n",
      "29/29 [==============================] - 0s 758us/step - loss: 20.9485 - mse: 20.9485 - val_loss: 19.9495 - val_mse: 19.9495\n",
      "Epoch 108/1000\n",
      "29/29 [==============================] - 0s 763us/step - loss: 20.9088 - mse: 20.9088 - val_loss: 20.0177 - val_mse: 20.0177\n",
      "Epoch 109/1000\n",
      "29/29 [==============================] - 0s 758us/step - loss: 20.8970 - mse: 20.8970 - val_loss: 19.9986 - val_mse: 19.9986\n",
      "Epoch 110/1000\n",
      "29/29 [==============================] - 0s 800us/step - loss: 20.8571 - mse: 20.8571 - val_loss: 19.9147 - val_mse: 19.9147\n",
      "Epoch 111/1000\n",
      "29/29 [==============================] - 0s 771us/step - loss: 20.8505 - mse: 20.8505 - val_loss: 19.9272 - val_mse: 19.9272\n",
      "Epoch 112/1000\n",
      "29/29 [==============================] - 0s 775us/step - loss: 20.8359 - mse: 20.8359 - val_loss: 19.8180 - val_mse: 19.8180\n",
      "Epoch 113/1000\n",
      "29/29 [==============================] - 0s 772us/step - loss: 20.8314 - mse: 20.8314 - val_loss: 19.8798 - val_mse: 19.8798\n",
      "Epoch 114/1000\n",
      "29/29 [==============================] - 0s 708us/step - loss: 20.8078 - mse: 20.8078 - val_loss: 19.7877 - val_mse: 19.7877\n",
      "Epoch 115/1000\n",
      "29/29 [==============================] - 0s 753us/step - loss: 20.8112 - mse: 20.8112 - val_loss: 19.7675 - val_mse: 19.7675\n",
      "Epoch 116/1000\n",
      "29/29 [==============================] - 0s 732us/step - loss: 20.7949 - mse: 20.7949 - val_loss: 19.7068 - val_mse: 19.7068\n",
      "Epoch 117/1000\n",
      "29/29 [==============================] - 0s 945us/step - loss: 20.7935 - mse: 20.7935 - val_loss: 19.8446 - val_mse: 19.8446\n",
      "Epoch 118/1000\n",
      "29/29 [==============================] - 0s 729us/step - loss: 20.7817 - mse: 20.7817 - val_loss: 19.8505 - val_mse: 19.8505\n",
      "Epoch 119/1000\n",
      "29/29 [==============================] - 0s 753us/step - loss: 20.7574 - mse: 20.7574 - val_loss: 19.7377 - val_mse: 19.7377\n",
      "Epoch 120/1000\n",
      "29/29 [==============================] - 0s 752us/step - loss: 20.7377 - mse: 20.7377 - val_loss: 19.7744 - val_mse: 19.7744\n",
      "Epoch 121/1000\n",
      "29/29 [==============================] - 0s 787us/step - loss: 20.7507 - mse: 20.7507 - val_loss: 19.7532 - val_mse: 19.7532\n",
      "Epoch 122/1000\n",
      "29/29 [==============================] - 0s 788us/step - loss: 20.7375 - mse: 20.7375 - val_loss: 19.7115 - val_mse: 19.7115\n",
      "Epoch 123/1000\n",
      "29/29 [==============================] - 0s 767us/step - loss: 20.7265 - mse: 20.7265 - val_loss: 19.6833 - val_mse: 19.6833\n",
      "Epoch 124/1000\n",
      "29/29 [==============================] - 0s 753us/step - loss: 20.6712 - mse: 20.6712 - val_loss: 19.6512 - val_mse: 19.6512\n",
      "Epoch 125/1000\n",
      "29/29 [==============================] - 0s 786us/step - loss: 20.6509 - mse: 20.6509 - val_loss: 19.6815 - val_mse: 19.6815\n",
      "Epoch 126/1000\n",
      "29/29 [==============================] - 0s 777us/step - loss: 20.6506 - mse: 20.6506 - val_loss: 19.5692 - val_mse: 19.5692\n",
      "Epoch 127/1000\n",
      "29/29 [==============================] - 0s 778us/step - loss: 20.6157 - mse: 20.6157 - val_loss: 19.5572 - val_mse: 19.5572\n",
      "Epoch 128/1000\n",
      "29/29 [==============================] - 0s 738us/step - loss: 20.5976 - mse: 20.5976 - val_loss: 19.5216 - val_mse: 19.5216\n",
      "Epoch 129/1000\n",
      "29/29 [==============================] - 0s 733us/step - loss: 20.5680 - mse: 20.5680 - val_loss: 19.5124 - val_mse: 19.5124\n",
      "Epoch 130/1000\n",
      "29/29 [==============================] - 0s 786us/step - loss: 20.5631 - mse: 20.5631 - val_loss: 19.4924 - val_mse: 19.4924\n",
      "Epoch 131/1000\n",
      "29/29 [==============================] - 0s 740us/step - loss: 20.5502 - mse: 20.5502 - val_loss: 19.5057 - val_mse: 19.5057\n",
      "Epoch 132/1000\n",
      "29/29 [==============================] - 0s 718us/step - loss: 20.5192 - mse: 20.5192 - val_loss: 19.4463 - val_mse: 19.4463\n",
      "Epoch 133/1000\n",
      "29/29 [==============================] - 0s 741us/step - loss: 20.5136 - mse: 20.5136 - val_loss: 19.3244 - val_mse: 19.3244\n",
      "Epoch 134/1000\n",
      "29/29 [==============================] - 0s 716us/step - loss: 20.4619 - mse: 20.4619 - val_loss: 19.3788 - val_mse: 19.3788\n",
      "Epoch 135/1000\n",
      "29/29 [==============================] - 0s 725us/step - loss: 20.4368 - mse: 20.4368 - val_loss: 19.3428 - val_mse: 19.3428\n",
      "Epoch 136/1000\n",
      "29/29 [==============================] - 0s 739us/step - loss: 20.4200 - mse: 20.4200 - val_loss: 19.4005 - val_mse: 19.4005\n",
      "Epoch 137/1000\n",
      "29/29 [==============================] - 0s 770us/step - loss: 20.4043 - mse: 20.4043 - val_loss: 19.2252 - val_mse: 19.2252\n",
      "Epoch 138/1000\n",
      "29/29 [==============================] - 0s 744us/step - loss: 20.3803 - mse: 20.3803 - val_loss: 19.3073 - val_mse: 19.3073\n",
      "Epoch 139/1000\n",
      "29/29 [==============================] - 0s 768us/step - loss: 20.3423 - mse: 20.3423 - val_loss: 19.1960 - val_mse: 19.1960\n",
      "Epoch 140/1000\n",
      "29/29 [==============================] - 0s 707us/step - loss: 20.3022 - mse: 20.3022 - val_loss: 19.2310 - val_mse: 19.2310\n",
      "Epoch 141/1000\n",
      "29/29 [==============================] - 0s 725us/step - loss: 20.3494 - mse: 20.3494 - val_loss: 19.3482 - val_mse: 19.3482\n",
      "Epoch 142/1000\n",
      "29/29 [==============================] - 0s 774us/step - loss: 20.2584 - mse: 20.2584 - val_loss: 19.2012 - val_mse: 19.2012\n",
      "Epoch 143/1000\n",
      "29/29 [==============================] - 0s 786us/step - loss: 20.2609 - mse: 20.2609 - val_loss: 19.0373 - val_mse: 19.0373\n",
      "Epoch 144/1000\n",
      "29/29 [==============================] - 0s 746us/step - loss: 20.1782 - mse: 20.1782 - val_loss: 19.0387 - val_mse: 19.0387\n",
      "Epoch 145/1000\n",
      "29/29 [==============================] - 0s 746us/step - loss: 20.1828 - mse: 20.1828 - val_loss: 18.9526 - val_mse: 18.9526\n",
      "Epoch 146/1000\n",
      "29/29 [==============================] - 0s 749us/step - loss: 20.1244 - mse: 20.1244 - val_loss: 18.8430 - val_mse: 18.8430\n",
      "Epoch 147/1000\n",
      "29/29 [==============================] - 0s 786us/step - loss: 20.0996 - mse: 20.0996 - val_loss: 19.0640 - val_mse: 19.0640\n",
      "Epoch 148/1000\n",
      "29/29 [==============================] - 0s 705us/step - loss: 20.0547 - mse: 20.0547 - val_loss: 18.7514 - val_mse: 18.7514\n",
      "Epoch 149/1000\n",
      "29/29 [==============================] - 0s 742us/step - loss: 19.9793 - mse: 19.9793 - val_loss: 18.7800 - val_mse: 18.7800\n",
      "Epoch 150/1000\n",
      "29/29 [==============================] - 0s 738us/step - loss: 19.9817 - mse: 19.9817 - val_loss: 18.7864 - val_mse: 18.7864\n",
      "Epoch 151/1000\n",
      "29/29 [==============================] - 0s 753us/step - loss: 19.9078 - mse: 19.9078 - val_loss: 18.6173 - val_mse: 18.6173\n",
      "Epoch 152/1000\n",
      "29/29 [==============================] - 0s 758us/step - loss: 19.8834 - mse: 19.8834 - val_loss: 18.6029 - val_mse: 18.6029\n",
      "Epoch 153/1000\n",
      "29/29 [==============================] - 0s 746us/step - loss: 19.8446 - mse: 19.8446 - val_loss: 18.5704 - val_mse: 18.5704\n",
      "Epoch 154/1000\n",
      "29/29 [==============================] - 0s 742us/step - loss: 19.8114 - mse: 19.8114 - val_loss: 18.4527 - val_mse: 18.4527\n",
      "Epoch 155/1000\n",
      "29/29 [==============================] - 0s 744us/step - loss: 19.7267 - mse: 19.7267 - val_loss: 18.3767 - val_mse: 18.3767\n",
      "Epoch 156/1000\n",
      "29/29 [==============================] - 0s 746us/step - loss: 19.7243 - mse: 19.7243 - val_loss: 18.4769 - val_mse: 18.4769\n",
      "Epoch 157/1000\n",
      "29/29 [==============================] - 0s 757us/step - loss: 19.6718 - mse: 19.6718 - val_loss: 18.2061 - val_mse: 18.2061\n",
      "Epoch 158/1000\n",
      "29/29 [==============================] - 0s 741us/step - loss: 19.6038 - mse: 19.6038 - val_loss: 18.1696 - val_mse: 18.1696\n",
      "Epoch 159/1000\n",
      "29/29 [==============================] - 0s 746us/step - loss: 19.5239 - mse: 19.5239 - val_loss: 17.9522 - val_mse: 17.9522\n",
      "Epoch 160/1000\n",
      "29/29 [==============================] - 0s 811us/step - loss: 19.4601 - mse: 19.4601 - val_loss: 18.0138 - val_mse: 18.0138\n",
      "Epoch 161/1000\n",
      "29/29 [==============================] - 0s 786us/step - loss: 19.3801 - mse: 19.3801 - val_loss: 18.0891 - val_mse: 18.0891\n",
      "Epoch 162/1000\n",
      "29/29 [==============================] - 0s 757us/step - loss: 19.3179 - mse: 19.3179 - val_loss: 17.9741 - val_mse: 17.9741\n",
      "Epoch 163/1000\n",
      "29/29 [==============================] - 0s 730us/step - loss: 19.2850 - mse: 19.2850 - val_loss: 17.9875 - val_mse: 17.9875\n",
      "Epoch 164/1000\n",
      "29/29 [==============================] - 0s 814us/step - loss: 19.1796 - mse: 19.1796 - val_loss: 17.7730 - val_mse: 17.7730\n",
      "Epoch 165/1000\n",
      "29/29 [==============================] - 0s 741us/step - loss: 19.1565 - mse: 19.1565 - val_loss: 17.7335 - val_mse: 17.7335\n",
      "Epoch 166/1000\n",
      "29/29 [==============================] - 0s 776us/step - loss: 19.0959 - mse: 19.0959 - val_loss: 17.6238 - val_mse: 17.6238\n",
      "Epoch 167/1000\n",
      "29/29 [==============================] - 0s 764us/step - loss: 19.0301 - mse: 19.0301 - val_loss: 17.6178 - val_mse: 17.6178\n",
      "Epoch 168/1000\n",
      "29/29 [==============================] - 0s 753us/step - loss: 18.9890 - mse: 18.9890 - val_loss: 17.4715 - val_mse: 17.4715\n",
      "Epoch 169/1000\n",
      "29/29 [==============================] - 0s 698us/step - loss: 18.9362 - mse: 18.9362 - val_loss: 17.4337 - val_mse: 17.4337\n",
      "Epoch 170/1000\n",
      "29/29 [==============================] - 0s 755us/step - loss: 18.9668 - mse: 18.9668 - val_loss: 17.3671 - val_mse: 17.3671\n",
      "Epoch 171/1000\n",
      "29/29 [==============================] - 0s 742us/step - loss: 18.8112 - mse: 18.8112 - val_loss: 17.2702 - val_mse: 17.2702\n",
      "Epoch 172/1000\n",
      "29/29 [==============================] - 0s 726us/step - loss: 18.7475 - mse: 18.7475 - val_loss: 17.2531 - val_mse: 17.2531\n",
      "Epoch 173/1000\n",
      "29/29 [==============================] - 0s 715us/step - loss: 18.6822 - mse: 18.6822 - val_loss: 17.0887 - val_mse: 17.0887\n",
      "Epoch 174/1000\n",
      "29/29 [==============================] - 0s 747us/step - loss: 18.6371 - mse: 18.6371 - val_loss: 16.9000 - val_mse: 16.9000\n",
      "Epoch 175/1000\n",
      "29/29 [==============================] - 0s 681us/step - loss: 18.6080 - mse: 18.6080 - val_loss: 16.9788 - val_mse: 16.9788\n",
      "Epoch 176/1000\n",
      "29/29 [==============================] - 0s 725us/step - loss: 18.5256 - mse: 18.5256 - val_loss: 17.0102 - val_mse: 17.0102\n",
      "Epoch 177/1000\n",
      "29/29 [==============================] - 0s 746us/step - loss: 18.4553 - mse: 18.4553 - val_loss: 16.7340 - val_mse: 16.7340\n",
      "Epoch 178/1000\n",
      "29/29 [==============================] - 0s 713us/step - loss: 18.4477 - mse: 18.4477 - val_loss: 16.9998 - val_mse: 16.9998\n",
      "Epoch 179/1000\n",
      "29/29 [==============================] - 0s 719us/step - loss: 18.3371 - mse: 18.3371 - val_loss: 16.8552 - val_mse: 16.8552\n",
      "Epoch 180/1000\n",
      "29/29 [==============================] - 0s 748us/step - loss: 18.2839 - mse: 18.2839 - val_loss: 16.4707 - val_mse: 16.4707\n",
      "Epoch 181/1000\n",
      "29/29 [==============================] - 0s 678us/step - loss: 18.2206 - mse: 18.2206 - val_loss: 16.5241 - val_mse: 16.5241\n",
      "Epoch 182/1000\n",
      "29/29 [==============================] - 0s 756us/step - loss: 18.1624 - mse: 18.1624 - val_loss: 16.4828 - val_mse: 16.4828\n",
      "Epoch 183/1000\n",
      "29/29 [==============================] - 0s 743us/step - loss: 18.1594 - mse: 18.1594 - val_loss: 16.5715 - val_mse: 16.5715\n",
      "Epoch 184/1000\n",
      "29/29 [==============================] - 0s 705us/step - loss: 18.0374 - mse: 18.0374 - val_loss: 16.2958 - val_mse: 16.2958\n",
      "Epoch 185/1000\n",
      "29/29 [==============================] - 0s 745us/step - loss: 18.0010 - mse: 18.0010 - val_loss: 16.2174 - val_mse: 16.2174\n",
      "Epoch 186/1000\n",
      "29/29 [==============================] - 0s 732us/step - loss: 17.9253 - mse: 17.9253 - val_loss: 16.3456 - val_mse: 16.3456\n",
      "Epoch 187/1000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 17.8809 - mse: 17.8809 - val_loss: 16.2196 - val_mse: 16.2196\n",
      "Epoch 188/1000\n",
      "29/29 [==============================] - 0s 741us/step - loss: 17.7901 - mse: 17.7901 - val_loss: 16.2512 - val_mse: 16.2512\n",
      "Epoch 189/1000\n",
      "29/29 [==============================] - 0s 698us/step - loss: 17.7355 - mse: 17.7355 - val_loss: 15.7491 - val_mse: 15.7491\n",
      "Epoch 190/1000\n",
      "29/29 [==============================] - 0s 719us/step - loss: 17.6133 - mse: 17.6133 - val_loss: 15.7115 - val_mse: 15.7115\n",
      "Epoch 191/1000\n",
      "29/29 [==============================] - 0s 739us/step - loss: 17.5899 - mse: 17.5899 - val_loss: 15.8249 - val_mse: 15.8249\n",
      "Epoch 192/1000\n",
      "29/29 [==============================] - 0s 689us/step - loss: 17.4956 - mse: 17.4956 - val_loss: 15.6377 - val_mse: 15.6377\n",
      "Epoch 193/1000\n",
      "29/29 [==============================] - 0s 718us/step - loss: 17.4975 - mse: 17.4975 - val_loss: 15.4113 - val_mse: 15.4113\n",
      "Epoch 194/1000\n",
      "29/29 [==============================] - 0s 724us/step - loss: 17.3219 - mse: 17.3219 - val_loss: 15.3989 - val_mse: 15.3989\n",
      "Epoch 195/1000\n",
      "29/29 [==============================] - 0s 735us/step - loss: 17.2985 - mse: 17.2985 - val_loss: 15.4608 - val_mse: 15.4608\n",
      "Epoch 196/1000\n",
      "29/29 [==============================] - 0s 794us/step - loss: 17.2492 - mse: 17.2492 - val_loss: 15.2051 - val_mse: 15.2051\n",
      "Epoch 197/1000\n",
      "29/29 [==============================] - 0s 787us/step - loss: 17.1394 - mse: 17.1394 - val_loss: 15.3209 - val_mse: 15.3209\n",
      "Epoch 198/1000\n",
      "29/29 [==============================] - 0s 803us/step - loss: 17.0845 - mse: 17.0845 - val_loss: 15.1648 - val_mse: 15.1648\n",
      "Epoch 199/1000\n",
      "29/29 [==============================] - 0s 799us/step - loss: 16.9620 - mse: 16.9620 - val_loss: 14.8372 - val_mse: 14.8372\n",
      "Epoch 200/1000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 16.9378 - mse: 16.9378 - val_loss: 14.7635 - val_mse: 14.7635\n",
      "Epoch 201/1000\n",
      "29/29 [==============================] - 0s 730us/step - loss: 16.8818 - mse: 16.8818 - val_loss: 14.8204 - val_mse: 14.8204\n",
      "Epoch 202/1000\n",
      "29/29 [==============================] - 0s 767us/step - loss: 16.8015 - mse: 16.8015 - val_loss: 14.8328 - val_mse: 14.8328\n",
      "Epoch 203/1000\n",
      "29/29 [==============================] - 0s 751us/step - loss: 16.6999 - mse: 16.6999 - val_loss: 14.7089 - val_mse: 14.7089\n",
      "Epoch 204/1000\n",
      "29/29 [==============================] - 0s 743us/step - loss: 16.5734 - mse: 16.5734 - val_loss: 14.4077 - val_mse: 14.4077\n",
      "Epoch 205/1000\n",
      "29/29 [==============================] - 0s 752us/step - loss: 16.5022 - mse: 16.5022 - val_loss: 14.4221 - val_mse: 14.4221\n",
      "Epoch 206/1000\n",
      "29/29 [==============================] - 0s 753us/step - loss: 16.4151 - mse: 16.4151 - val_loss: 14.5191 - val_mse: 14.5191\n",
      "Epoch 207/1000\n",
      "29/29 [==============================] - 0s 726us/step - loss: 16.3259 - mse: 16.3259 - val_loss: 14.2081 - val_mse: 14.2081\n",
      "Epoch 208/1000\n",
      "29/29 [==============================] - 0s 707us/step - loss: 16.2527 - mse: 16.2527 - val_loss: 14.2592 - val_mse: 14.2592\n",
      "Epoch 209/1000\n",
      "29/29 [==============================] - 0s 792us/step - loss: 16.1854 - mse: 16.1854 - val_loss: 14.0926 - val_mse: 14.0926\n",
      "Epoch 210/1000\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 16.1421 - mse: 16.1421 - val_loss: 14.2165 - val_mse: 14.2165\n",
      "Epoch 211/1000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 16.0000 - mse: 16.0000 - val_loss: 14.0319 - val_mse: 14.0319\n",
      "Epoch 212/1000\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 16.0137 - mse: 16.0137 - val_loss: 13.6449 - val_mse: 13.6449\n",
      "Epoch 213/1000\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 15.7971 - mse: 15.7971 - val_loss: 13.6301 - val_mse: 13.6301\n",
      "Epoch 214/1000\n",
      "29/29 [==============================] - 0s 961us/step - loss: 15.8268 - mse: 15.8268 - val_loss: 13.5129 - val_mse: 13.5129\n",
      "Epoch 215/1000\n",
      "29/29 [==============================] - 0s 856us/step - loss: 15.6389 - mse: 15.6389 - val_loss: 13.4612 - val_mse: 13.4612\n",
      "Epoch 216/1000\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 15.5582 - mse: 15.5582 - val_loss: 13.1774 - val_mse: 13.1774\n",
      "Epoch 217/1000\n",
      "29/29 [==============================] - 0s 917us/step - loss: 15.5964 - mse: 15.5964 - val_loss: 13.3001 - val_mse: 13.3001\n",
      "Epoch 218/1000\n",
      "29/29 [==============================] - 0s 837us/step - loss: 15.4254 - mse: 15.4254 - val_loss: 12.9510 - val_mse: 12.9510\n",
      "Epoch 219/1000\n",
      "29/29 [==============================] - 0s 786us/step - loss: 15.3095 - mse: 15.3095 - val_loss: 13.1046 - val_mse: 13.1046\n",
      "Epoch 220/1000\n",
      "29/29 [==============================] - 0s 983us/step - loss: 15.1996 - mse: 15.1996 - val_loss: 12.7871 - val_mse: 12.7871\n",
      "Epoch 221/1000\n",
      "29/29 [==============================] - 0s 849us/step - loss: 15.1006 - mse: 15.1006 - val_loss: 12.7656 - val_mse: 12.7656\n",
      "Epoch 222/1000\n",
      "29/29 [==============================] - 0s 898us/step - loss: 15.1559 - mse: 15.1559 - val_loss: 12.7392 - val_mse: 12.7392\n",
      "Epoch 223/1000\n",
      "29/29 [==============================] - 0s 754us/step - loss: 14.9919 - mse: 14.9919 - val_loss: 12.6005 - val_mse: 12.6005\n",
      "Epoch 224/1000\n",
      "29/29 [==============================] - 0s 826us/step - loss: 14.8756 - mse: 14.8756 - val_loss: 12.4013 - val_mse: 12.4013\n",
      "Epoch 225/1000\n",
      "29/29 [==============================] - 0s 797us/step - loss: 14.7708 - mse: 14.7708 - val_loss: 12.7709 - val_mse: 12.7709\n",
      "Epoch 226/1000\n",
      "29/29 [==============================] - 0s 777us/step - loss: 14.6465 - mse: 14.6465 - val_loss: 12.3080 - val_mse: 12.3080\n",
      "Epoch 227/1000\n",
      "29/29 [==============================] - 0s 803us/step - loss: 14.5479 - mse: 14.5479 - val_loss: 12.1764 - val_mse: 12.1764\n",
      "Epoch 228/1000\n",
      "29/29 [==============================] - 0s 795us/step - loss: 14.5198 - mse: 14.5198 - val_loss: 11.9859 - val_mse: 11.9859\n",
      "Epoch 229/1000\n",
      "29/29 [==============================] - 0s 767us/step - loss: 14.3502 - mse: 14.3502 - val_loss: 12.1408 - val_mse: 12.1408\n",
      "Epoch 230/1000\n",
      "29/29 [==============================] - 0s 751us/step - loss: 14.2780 - mse: 14.2780 - val_loss: 11.8672 - val_mse: 11.8672\n",
      "Epoch 231/1000\n",
      "29/29 [==============================] - 0s 810us/step - loss: 14.2324 - mse: 14.2324 - val_loss: 11.8013 - val_mse: 11.8013\n",
      "Epoch 232/1000\n",
      "29/29 [==============================] - 0s 742us/step - loss: 14.1421 - mse: 14.1421 - val_loss: 11.8316 - val_mse: 11.8316\n",
      "Epoch 233/1000\n",
      "29/29 [==============================] - 0s 753us/step - loss: 13.9601 - mse: 13.9601 - val_loss: 11.8134 - val_mse: 11.8134\n",
      "Epoch 234/1000\n",
      "29/29 [==============================] - 0s 775us/step - loss: 14.0055 - mse: 14.0055 - val_loss: 11.6497 - val_mse: 11.6497\n",
      "Epoch 235/1000\n",
      "29/29 [==============================] - 0s 897us/step - loss: 13.8142 - mse: 13.8142 - val_loss: 11.2348 - val_mse: 11.2348\n",
      "Epoch 236/1000\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 13.7163 - mse: 13.7163 - val_loss: 11.4562 - val_mse: 11.4562\n",
      "Epoch 237/1000\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 13.7094 - mse: 13.7094 - val_loss: 11.3690 - val_mse: 11.3690\n",
      "Epoch 238/1000\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 13.6327 - mse: 13.6327 - val_loss: 11.2621 - val_mse: 11.2621\n",
      "Epoch 239/1000\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 13.4109 - mse: 13.4109 - val_loss: 11.2381 - val_mse: 11.2381\n",
      "Epoch 240/1000\n",
      "29/29 [==============================] - 0s 909us/step - loss: 13.2867 - mse: 13.2867 - val_loss: 10.9435 - val_mse: 10.9435\n",
      "Epoch 241/1000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 13.2108 - mse: 13.2108 - val_loss: 10.5893 - val_mse: 10.5893\n",
      "Epoch 242/1000\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 13.1618 - mse: 13.1618 - val_loss: 10.8043 - val_mse: 10.8043\n",
      "Epoch 243/1000\n",
      "29/29 [==============================] - 0s 996us/step - loss: 13.1374 - mse: 13.1374 - val_loss: 10.6707 - val_mse: 10.6707\n",
      "Epoch 244/1000\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 12.9034 - mse: 12.9034 - val_loss: 10.3294 - val_mse: 10.3294\n",
      "Epoch 245/1000\n",
      "29/29 [==============================] - 0s 764us/step - loss: 12.7901 - mse: 12.7901 - val_loss: 10.6483 - val_mse: 10.6483\n",
      "Epoch 246/1000\n",
      "29/29 [==============================] - 0s 763us/step - loss: 12.7762 - mse: 12.7762 - val_loss: 10.6026 - val_mse: 10.6026\n",
      "Epoch 247/1000\n",
      "29/29 [==============================] - 0s 740us/step - loss: 12.6829 - mse: 12.6829 - val_loss: 10.2181 - val_mse: 10.2181\n",
      "Epoch 248/1000\n",
      "29/29 [==============================] - 0s 749us/step - loss: 12.5183 - mse: 12.5183 - val_loss: 9.9605 - val_mse: 9.9605\n",
      "Epoch 249/1000\n",
      "29/29 [==============================] - 0s 759us/step - loss: 12.4120 - mse: 12.4120 - val_loss: 10.0898 - val_mse: 10.0898\n",
      "Epoch 250/1000\n",
      "29/29 [==============================] - 0s 783us/step - loss: 12.3484 - mse: 12.3484 - val_loss: 10.0916 - val_mse: 10.0916\n",
      "Epoch 251/1000\n",
      "29/29 [==============================] - 0s 740us/step - loss: 12.2478 - mse: 12.2478 - val_loss: 9.6662 - val_mse: 9.6662\n",
      "Epoch 252/1000\n",
      "29/29 [==============================] - 0s 754us/step - loss: 12.2631 - mse: 12.2631 - val_loss: 9.4552 - val_mse: 9.4552\n",
      "Epoch 253/1000\n",
      "29/29 [==============================] - 0s 732us/step - loss: 12.2120 - mse: 12.2120 - val_loss: 10.0560 - val_mse: 10.0560\n",
      "Epoch 254/1000\n",
      "29/29 [==============================] - 0s 752us/step - loss: 12.0038 - mse: 12.0038 - val_loss: 9.3033 - val_mse: 9.3033\n",
      "Epoch 255/1000\n",
      "29/29 [==============================] - 0s 760us/step - loss: 11.9503 - mse: 11.9503 - val_loss: 9.6547 - val_mse: 9.6547\n",
      "Epoch 256/1000\n",
      "29/29 [==============================] - 0s 867us/step - loss: 11.8621 - mse: 11.8621 - val_loss: 9.2888 - val_mse: 9.2888\n",
      "Epoch 257/1000\n",
      "29/29 [==============================] - 0s 751us/step - loss: 11.7506 - mse: 11.7506 - val_loss: 9.2075 - val_mse: 9.2075\n",
      "Epoch 258/1000\n",
      "29/29 [==============================] - 0s 741us/step - loss: 11.6141 - mse: 11.6141 - val_loss: 9.5669 - val_mse: 9.5669\n",
      "Epoch 259/1000\n",
      "29/29 [==============================] - 0s 725us/step - loss: 11.5574 - mse: 11.5574 - val_loss: 9.1853 - val_mse: 9.1853\n",
      "Epoch 260/1000\n",
      "29/29 [==============================] - 0s 708us/step - loss: 11.4234 - mse: 11.4234 - val_loss: 8.9837 - val_mse: 8.9837\n",
      "Epoch 261/1000\n",
      "29/29 [==============================] - 0s 695us/step - loss: 11.3707 - mse: 11.3707 - val_loss: 9.0470 - val_mse: 9.0470\n",
      "Epoch 262/1000\n",
      "29/29 [==============================] - 0s 727us/step - loss: 11.3278 - mse: 11.3278 - val_loss: 9.1196 - val_mse: 9.1196\n",
      "Epoch 263/1000\n",
      "29/29 [==============================] - 0s 733us/step - loss: 11.2295 - mse: 11.2295 - val_loss: 9.0047 - val_mse: 9.0047\n",
      "Epoch 264/1000\n",
      "29/29 [==============================] - 0s 740us/step - loss: 11.1246 - mse: 11.1246 - val_loss: 8.4377 - val_mse: 8.4377\n",
      "Epoch 265/1000\n",
      "29/29 [==============================] - 0s 726us/step - loss: 11.0459 - mse: 11.0459 - val_loss: 8.9431 - val_mse: 8.9431\n",
      "Epoch 266/1000\n",
      "29/29 [==============================] - 0s 691us/step - loss: 11.0231 - mse: 11.0231 - val_loss: 8.6590 - val_mse: 8.6590\n",
      "Epoch 267/1000\n",
      "29/29 [==============================] - 0s 743us/step - loss: 10.8189 - mse: 10.8189 - val_loss: 8.5756 - val_mse: 8.5756\n",
      "Epoch 268/1000\n",
      "29/29 [==============================] - 0s 734us/step - loss: 10.7352 - mse: 10.7352 - val_loss: 8.3240 - val_mse: 8.3240\n",
      "Epoch 269/1000\n",
      "29/29 [==============================] - 0s 774us/step - loss: 10.6590 - mse: 10.6590 - val_loss: 8.3473 - val_mse: 8.3473\n",
      "Epoch 270/1000\n",
      "29/29 [==============================] - 0s 769us/step - loss: 10.6074 - mse: 10.6074 - val_loss: 8.4524 - val_mse: 8.4524\n",
      "Epoch 271/1000\n",
      "29/29 [==============================] - 0s 727us/step - loss: 10.5214 - mse: 10.5214 - val_loss: 8.3390 - val_mse: 8.3390\n",
      "Epoch 272/1000\n",
      "29/29 [==============================] - 0s 756us/step - loss: 10.4415 - mse: 10.4415 - val_loss: 8.1806 - val_mse: 8.1806\n",
      "Epoch 273/1000\n",
      "29/29 [==============================] - 0s 792us/step - loss: 10.3227 - mse: 10.3227 - val_loss: 8.1722 - val_mse: 8.1722\n",
      "Epoch 274/1000\n",
      "29/29 [==============================] - 0s 750us/step - loss: 10.3030 - mse: 10.3030 - val_loss: 7.8854 - val_mse: 7.8854\n",
      "Epoch 275/1000\n",
      "29/29 [==============================] - 0s 727us/step - loss: 10.0402 - mse: 10.0402 - val_loss: 7.7871 - val_mse: 7.7871\n",
      "Epoch 276/1000\n",
      "29/29 [==============================] - 0s 770us/step - loss: 10.0004 - mse: 10.0004 - val_loss: 7.6457 - val_mse: 7.6457\n",
      "Epoch 277/1000\n",
      "29/29 [==============================] - 0s 888us/step - loss: 9.9287 - mse: 9.9287 - val_loss: 7.6725 - val_mse: 7.6725\n",
      "Epoch 278/1000\n",
      "29/29 [==============================] - 0s 742us/step - loss: 9.7762 - mse: 9.7762 - val_loss: 7.9643 - val_mse: 7.9643\n",
      "Epoch 279/1000\n",
      "29/29 [==============================] - 0s 746us/step - loss: 9.7921 - mse: 9.7921 - val_loss: 7.4619 - val_mse: 7.4619\n",
      "Epoch 280/1000\n",
      "29/29 [==============================] - 0s 774us/step - loss: 9.7115 - mse: 9.7115 - val_loss: 7.6971 - val_mse: 7.6971\n",
      "Epoch 281/1000\n",
      "29/29 [==============================] - 0s 749us/step - loss: 9.5711 - mse: 9.5711 - val_loss: 7.3060 - val_mse: 7.3060\n",
      "Epoch 282/1000\n",
      "29/29 [==============================] - 0s 741us/step - loss: 9.4847 - mse: 9.4847 - val_loss: 7.2295 - val_mse: 7.2295\n",
      "Epoch 283/1000\n",
      "29/29 [==============================] - 0s 759us/step - loss: 9.3714 - mse: 9.3714 - val_loss: 7.3885 - val_mse: 7.3885\n",
      "Epoch 284/1000\n",
      "29/29 [==============================] - 0s 785us/step - loss: 9.3383 - mse: 9.3383 - val_loss: 7.1785 - val_mse: 7.1785\n",
      "Epoch 285/1000\n",
      "29/29 [==============================] - 0s 753us/step - loss: 9.2801 - mse: 9.2801 - val_loss: 7.1435 - val_mse: 7.1435\n",
      "Epoch 286/1000\n",
      "29/29 [==============================] - 0s 753us/step - loss: 9.2009 - mse: 9.2009 - val_loss: 6.7036 - val_mse: 6.7036\n",
      "Epoch 287/1000\n",
      "29/29 [==============================] - 0s 789us/step - loss: 9.1030 - mse: 9.1030 - val_loss: 7.0780 - val_mse: 7.0780\n",
      "Epoch 288/1000\n",
      "29/29 [==============================] - 0s 728us/step - loss: 8.9912 - mse: 8.9912 - val_loss: 6.7214 - val_mse: 6.7214\n",
      "Epoch 289/1000\n",
      "29/29 [==============================] - 0s 775us/step - loss: 8.9049 - mse: 8.9049 - val_loss: 6.9148 - val_mse: 6.9148\n",
      "Epoch 290/1000\n",
      "29/29 [==============================] - 0s 759us/step - loss: 8.8958 - mse: 8.8958 - val_loss: 6.5095 - val_mse: 6.5095\n",
      "Epoch 291/1000\n",
      "29/29 [==============================] - 0s 706us/step - loss: 8.8607 - mse: 8.8607 - val_loss: 6.4538 - val_mse: 6.4538\n",
      "Epoch 292/1000\n",
      "29/29 [==============================] - 0s 769us/step - loss: 8.6149 - mse: 8.6149 - val_loss: 6.8527 - val_mse: 6.8527\n",
      "Epoch 293/1000\n",
      "29/29 [==============================] - 0s 802us/step - loss: 8.5290 - mse: 8.5290 - val_loss: 6.4007 - val_mse: 6.4007\n",
      "Epoch 294/1000\n",
      "29/29 [==============================] - 0s 748us/step - loss: 8.4741 - mse: 8.4741 - val_loss: 6.6867 - val_mse: 6.6867\n",
      "Epoch 295/1000\n",
      "29/29 [==============================] - 0s 749us/step - loss: 8.5618 - mse: 8.5618 - val_loss: 6.5002 - val_mse: 6.5002\n",
      "Epoch 296/1000\n",
      "29/29 [==============================] - 0s 766us/step - loss: 8.4075 - mse: 8.4075 - val_loss: 6.6179 - val_mse: 6.6179\n",
      "Epoch 297/1000\n",
      "29/29 [==============================] - 0s 758us/step - loss: 8.3509 - mse: 8.3509 - val_loss: 6.2121 - val_mse: 6.2121\n",
      "Epoch 298/1000\n",
      "29/29 [==============================] - 0s 809us/step - loss: 8.1484 - mse: 8.1484 - val_loss: 6.2673 - val_mse: 6.2673\n",
      "Epoch 299/1000\n",
      "29/29 [==============================] - 0s 760us/step - loss: 8.1206 - mse: 8.1206 - val_loss: 6.1134 - val_mse: 6.1134\n",
      "Epoch 300/1000\n",
      "29/29 [==============================] - 0s 821us/step - loss: 8.0850 - mse: 8.0850 - val_loss: 5.9864 - val_mse: 5.9864\n",
      "Epoch 301/1000\n",
      "29/29 [==============================] - 0s 782us/step - loss: 8.0621 - mse: 8.0621 - val_loss: 6.3514 - val_mse: 6.3514\n",
      "Epoch 302/1000\n",
      "29/29 [==============================] - 0s 774us/step - loss: 7.8743 - mse: 7.8743 - val_loss: 5.7948 - val_mse: 5.7948\n",
      "Epoch 303/1000\n",
      "29/29 [==============================] - 0s 754us/step - loss: 7.8729 - mse: 7.8729 - val_loss: 6.3481 - val_mse: 6.3481\n",
      "Epoch 304/1000\n",
      "29/29 [==============================] - 0s 754us/step - loss: 7.6613 - mse: 7.6613 - val_loss: 5.7863 - val_mse: 5.7863\n",
      "Epoch 305/1000\n",
      "29/29 [==============================] - 0s 762us/step - loss: 7.6456 - mse: 7.6456 - val_loss: 5.6537 - val_mse: 5.6537\n",
      "Epoch 306/1000\n",
      "29/29 [==============================] - 0s 683us/step - loss: 7.5834 - mse: 7.5834 - val_loss: 5.9067 - val_mse: 5.9067\n",
      "Epoch 307/1000\n",
      "29/29 [==============================] - 0s 748us/step - loss: 7.5050 - mse: 7.5050 - val_loss: 5.6337 - val_mse: 5.6337\n",
      "Epoch 308/1000\n",
      "29/29 [==============================] - 0s 801us/step - loss: 7.4651 - mse: 7.4651 - val_loss: 6.0467 - val_mse: 6.0467\n",
      "Epoch 309/1000\n",
      "29/29 [==============================] - 0s 703us/step - loss: 7.3695 - mse: 7.3695 - val_loss: 5.6936 - val_mse: 5.6936\n",
      "Epoch 310/1000\n",
      "29/29 [==============================] - 0s 754us/step - loss: 7.2834 - mse: 7.2834 - val_loss: 5.8399 - val_mse: 5.8399\n",
      "Epoch 311/1000\n",
      "29/29 [==============================] - 0s 754us/step - loss: 7.1597 - mse: 7.1597 - val_loss: 5.4227 - val_mse: 5.4227\n",
      "Epoch 312/1000\n",
      "29/29 [==============================] - 0s 706us/step - loss: 7.1521 - mse: 7.1521 - val_loss: 5.2152 - val_mse: 5.2152\n",
      "Epoch 313/1000\n",
      "29/29 [==============================] - 0s 748us/step - loss: 7.1222 - mse: 7.1222 - val_loss: 5.5233 - val_mse: 5.5233\n",
      "Epoch 314/1000\n",
      "29/29 [==============================] - 0s 773us/step - loss: 6.9266 - mse: 6.9266 - val_loss: 5.2747 - val_mse: 5.2747\n",
      "Epoch 315/1000\n",
      "29/29 [==============================] - 0s 733us/step - loss: 6.8763 - mse: 6.8763 - val_loss: 5.4548 - val_mse: 5.4548\n",
      "Epoch 316/1000\n",
      "29/29 [==============================] - 0s 811us/step - loss: 6.7793 - mse: 6.7793 - val_loss: 5.3414 - val_mse: 5.3414\n",
      "Epoch 317/1000\n",
      "29/29 [==============================] - 0s 753us/step - loss: 6.7511 - mse: 6.7511 - val_loss: 5.0945 - val_mse: 5.0945\n",
      "Epoch 318/1000\n",
      "29/29 [==============================] - 0s 787us/step - loss: 6.6399 - mse: 6.6399 - val_loss: 5.3720 - val_mse: 5.3720\n",
      "Epoch 319/1000\n",
      "29/29 [==============================] - 0s 745us/step - loss: 6.5552 - mse: 6.5552 - val_loss: 5.1350 - val_mse: 5.1350\n",
      "Epoch 320/1000\n",
      "29/29 [==============================] - 0s 823us/step - loss: 6.5444 - mse: 6.5444 - val_loss: 5.1333 - val_mse: 5.1333\n",
      "Epoch 321/1000\n",
      "29/29 [==============================] - 0s 851us/step - loss: 6.4349 - mse: 6.4349 - val_loss: 5.1638 - val_mse: 5.1638\n",
      "Epoch 322/1000\n",
      "29/29 [==============================] - 0s 809us/step - loss: 6.3747 - mse: 6.3747 - val_loss: 4.6333 - val_mse: 4.6333\n",
      "Epoch 323/1000\n",
      "29/29 [==============================] - 0s 824us/step - loss: 6.2783 - mse: 6.2783 - val_loss: 5.2063 - val_mse: 5.2063\n",
      "Epoch 324/1000\n",
      "29/29 [==============================] - 0s 847us/step - loss: 6.4561 - mse: 6.4561 - val_loss: 4.4018 - val_mse: 4.4018\n",
      "Epoch 325/1000\n",
      "29/29 [==============================] - 0s 845us/step - loss: 6.5310 - mse: 6.5310 - val_loss: 4.9564 - val_mse: 4.9564\n",
      "Epoch 326/1000\n",
      "29/29 [==============================] - 0s 853us/step - loss: 6.0186 - mse: 6.0186 - val_loss: 4.5782 - val_mse: 4.5782\n",
      "Epoch 327/1000\n",
      "29/29 [==============================] - 0s 779us/step - loss: 5.9985 - mse: 5.9985 - val_loss: 4.6587 - val_mse: 4.6587\n",
      "Epoch 328/1000\n",
      "29/29 [==============================] - 0s 825us/step - loss: 5.9290 - mse: 5.9290 - val_loss: 4.7684 - val_mse: 4.7684\n",
      "Epoch 329/1000\n",
      "29/29 [==============================] - 0s 840us/step - loss: 5.9100 - mse: 5.9100 - val_loss: 4.4070 - val_mse: 4.4070\n",
      "Epoch 330/1000\n",
      "29/29 [==============================] - 0s 942us/step - loss: 5.8784 - mse: 5.8784 - val_loss: 4.3950 - val_mse: 4.3950\n",
      "Epoch 331/1000\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 5.7407 - mse: 5.7407 - val_loss: 4.2707 - val_mse: 4.2707\n",
      "Epoch 332/1000\n",
      "29/29 [==============================] - 0s 786us/step - loss: 5.7794 - mse: 5.7794 - val_loss: 4.2208 - val_mse: 4.2208\n",
      "Epoch 333/1000\n",
      "29/29 [==============================] - 0s 765us/step - loss: 5.6067 - mse: 5.6067 - val_loss: 4.2140 - val_mse: 4.2140\n",
      "Epoch 334/1000\n",
      "29/29 [==============================] - 0s 800us/step - loss: 5.5011 - mse: 5.5011 - val_loss: 4.6879 - val_mse: 4.6879\n",
      "Epoch 335/1000\n",
      "29/29 [==============================] - 0s 729us/step - loss: 5.4963 - mse: 5.4963 - val_loss: 4.9323 - val_mse: 4.9323\n",
      "Epoch 336/1000\n",
      "29/29 [==============================] - 0s 740us/step - loss: 5.4623 - mse: 5.4623 - val_loss: 4.0845 - val_mse: 4.0845\n",
      "Epoch 337/1000\n",
      "29/29 [==============================] - 0s 775us/step - loss: 5.2729 - mse: 5.2729 - val_loss: 4.5688 - val_mse: 4.5688\n",
      "Epoch 338/1000\n",
      "29/29 [==============================] - 0s 718us/step - loss: 5.2984 - mse: 5.2984 - val_loss: 3.9288 - val_mse: 3.9288\n",
      "Epoch 339/1000\n",
      "29/29 [==============================] - 0s 814us/step - loss: 5.1765 - mse: 5.1765 - val_loss: 4.4995 - val_mse: 4.4995\n",
      "Epoch 340/1000\n",
      "29/29 [==============================] - 0s 827us/step - loss: 5.1826 - mse: 5.1826 - val_loss: 4.2932 - val_mse: 4.2932\n",
      "Epoch 341/1000\n",
      "29/29 [==============================] - 0s 939us/step - loss: 5.0428 - mse: 5.0428 - val_loss: 4.4874 - val_mse: 4.4874\n",
      "Epoch 342/1000\n",
      "29/29 [==============================] - 0s 792us/step - loss: 5.0135 - mse: 5.0135 - val_loss: 4.1593 - val_mse: 4.1593\n",
      "Epoch 343/1000\n",
      "29/29 [==============================] - 0s 799us/step - loss: 4.8860 - mse: 4.8860 - val_loss: 3.9310 - val_mse: 3.9310\n",
      "Epoch 344/1000\n",
      "29/29 [==============================] - 0s 801us/step - loss: 4.8066 - mse: 4.8066 - val_loss: 4.4989 - val_mse: 4.4989\n",
      "Epoch 345/1000\n",
      "29/29 [==============================] - 0s 814us/step - loss: 5.1302 - mse: 5.1302 - val_loss: 3.4730 - val_mse: 3.4730\n",
      "Epoch 346/1000\n",
      "29/29 [==============================] - 0s 731us/step - loss: 4.6549 - mse: 4.6549 - val_loss: 4.3663 - val_mse: 4.3663\n",
      "Epoch 347/1000\n",
      "29/29 [==============================] - 0s 757us/step - loss: 4.6922 - mse: 4.6922 - val_loss: 3.9076 - val_mse: 3.9076\n",
      "Epoch 348/1000\n",
      "29/29 [==============================] - 0s 768us/step - loss: 4.5298 - mse: 4.5298 - val_loss: 3.7375 - val_mse: 3.7375\n",
      "Epoch 349/1000\n",
      "29/29 [==============================] - 0s 802us/step - loss: 4.5254 - mse: 4.5254 - val_loss: 3.6007 - val_mse: 3.6007\n",
      "Epoch 350/1000\n",
      "29/29 [==============================] - 0s 825us/step - loss: 4.4132 - mse: 4.4132 - val_loss: 4.0996 - val_mse: 4.0996\n",
      "Epoch 351/1000\n",
      "29/29 [==============================] - 0s 817us/step - loss: 4.4233 - mse: 4.4233 - val_loss: 3.5595 - val_mse: 3.5595\n",
      "Epoch 352/1000\n",
      "29/29 [==============================] - 0s 806us/step - loss: 4.2716 - mse: 4.2716 - val_loss: 3.7866 - val_mse: 3.7866\n",
      "Epoch 353/1000\n",
      "29/29 [==============================] - 0s 812us/step - loss: 4.2309 - mse: 4.2309 - val_loss: 3.6030 - val_mse: 3.6030\n",
      "Epoch 354/1000\n",
      "29/29 [==============================] - 0s 802us/step - loss: 4.2171 - mse: 4.2171 - val_loss: 3.4518 - val_mse: 3.4518\n",
      "Epoch 355/1000\n",
      "29/29 [==============================] - 0s 859us/step - loss: 4.0819 - mse: 4.0819 - val_loss: 3.8995 - val_mse: 3.8995\n",
      "Epoch 356/1000\n",
      "29/29 [==============================] - 0s 811us/step - loss: 4.0717 - mse: 4.0717 - val_loss: 3.4047 - val_mse: 3.4047\n",
      "Epoch 357/1000\n",
      "29/29 [==============================] - 0s 809us/step - loss: 4.0129 - mse: 4.0129 - val_loss: 3.3090 - val_mse: 3.3090\n",
      "Epoch 358/1000\n",
      "29/29 [==============================] - 0s 783us/step - loss: 3.9458 - mse: 3.9458 - val_loss: 3.8221 - val_mse: 3.8221\n",
      "Epoch 359/1000\n",
      "29/29 [==============================] - 0s 848us/step - loss: 3.8748 - mse: 3.8748 - val_loss: 3.4312 - val_mse: 3.4312\n",
      "Epoch 360/1000\n",
      "29/29 [==============================] - 0s 732us/step - loss: 3.8822 - mse: 3.8822 - val_loss: 2.9556 - val_mse: 2.9556\n",
      "Epoch 361/1000\n",
      "29/29 [==============================] - 0s 765us/step - loss: 3.8330 - mse: 3.8330 - val_loss: 3.2040 - val_mse: 3.2040\n",
      "Epoch 362/1000\n",
      "29/29 [==============================] - 0s 846us/step - loss: 3.6862 - mse: 3.6862 - val_loss: 2.9216 - val_mse: 2.9216\n",
      "Epoch 363/1000\n",
      "29/29 [==============================] - 0s 858us/step - loss: 3.6726 - mse: 3.6726 - val_loss: 3.0286 - val_mse: 3.0286\n",
      "Epoch 364/1000\n",
      "29/29 [==============================] - 0s 871us/step - loss: 3.5841 - mse: 3.5841 - val_loss: 3.5513 - val_mse: 3.5513\n",
      "Epoch 365/1000\n",
      "29/29 [==============================] - 0s 881us/step - loss: 3.5821 - mse: 3.5821 - val_loss: 2.9112 - val_mse: 2.9112\n",
      "Epoch 366/1000\n",
      "29/29 [==============================] - 0s 809us/step - loss: 3.4503 - mse: 3.4503 - val_loss: 3.0940 - val_mse: 3.0940\n",
      "Epoch 367/1000\n",
      "29/29 [==============================] - 0s 788us/step - loss: 3.4758 - mse: 3.4758 - val_loss: 4.0817 - val_mse: 4.0817\n",
      "Epoch 368/1000\n",
      "29/29 [==============================] - 0s 844us/step - loss: 3.4955 - mse: 3.4955 - val_loss: 2.9302 - val_mse: 2.9302\n",
      "Epoch 369/1000\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 3.3286 - mse: 3.3286 - val_loss: 2.7388 - val_mse: 2.7388\n",
      "Epoch 370/1000\n",
      " 1/29 [>.............................] - ETA: 0s - loss: 2.2450 - mse: 2.2450"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# training_history = model.fit(features, labels, epochs=epochs, batch_size=batch_size)\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/nyu2023summerml1-PUBzEmvh-py3.10/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/nyu2023summerml1-PUBzEmvh-py3.10/lib/python3.10/site-packages/keras/src/engine/training.py:1791\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1775\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_eval_data_handler\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1776\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_eval_data_handler \u001B[38;5;241m=\u001B[39m data_adapter\u001B[38;5;241m.\u001B[39mget_data_handler(\n\u001B[1;32m   1777\u001B[0m         x\u001B[38;5;241m=\u001B[39mval_x,\n\u001B[1;32m   1778\u001B[0m         y\u001B[38;5;241m=\u001B[39mval_y,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1789\u001B[0m         pss_evaluation_shards\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pss_evaluation_shards,\n\u001B[1;32m   1790\u001B[0m     )\n\u001B[0;32m-> 1791\u001B[0m val_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1792\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_x\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1793\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_y\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1794\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_sample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_batch_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1796\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_queue_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1800\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_multiprocessing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1801\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1802\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_use_cached_eval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1803\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1804\u001B[0m val_logs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m   1805\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m name: val \u001B[38;5;28;01mfor\u001B[39;00m name, val \u001B[38;5;129;01min\u001B[39;00m val_logs\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m   1806\u001B[0m }\n\u001B[1;32m   1807\u001B[0m epoch_logs\u001B[38;5;241m.\u001B[39mupdate(val_logs)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/nyu2023summerml1-PUBzEmvh-py3.10/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/nyu2023summerml1-PUBzEmvh-py3.10/lib/python3.10/site-packages/keras/src/engine/training.py:2189\u001B[0m, in \u001B[0;36mModel.evaluate\u001B[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001B[0m\n\u001B[1;32m   2187\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_test_counter\u001B[38;5;241m.\u001B[39massign(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m   2188\u001B[0m callbacks\u001B[38;5;241m.\u001B[39mon_test_begin()\n\u001B[0;32m-> 2189\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (\n\u001B[1;32m   2190\u001B[0m     _,\n\u001B[1;32m   2191\u001B[0m     dataset_or_iterator,\n\u001B[1;32m   2192\u001B[0m ) \u001B[38;5;129;01min\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39menumerate_epochs():  \u001B[38;5;66;03m# Single epoch.\u001B[39;00m\n\u001B[1;32m   2193\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreset_metrics()\n\u001B[1;32m   2194\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mcatch_stop_iteration():\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/nyu2023summerml1-PUBzEmvh-py3.10/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1331\u001B[0m, in \u001B[0;36mDataHandler.enumerate_epochs\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1329\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001B[39;00m\n\u001B[1;32m   1330\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_truncate_execution_to_epoch():\n\u001B[0;32m-> 1331\u001B[0m     data_iterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1332\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_initial_epoch, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_epochs):\n\u001B[1;32m   1333\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_insufficient_data:  \u001B[38;5;66;03m# Set by `catch_stop_iteration`.\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/nyu2023summerml1-PUBzEmvh-py3.10/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:506\u001B[0m, in \u001B[0;36mDatasetV2.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    504\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m context\u001B[38;5;241m.\u001B[39mexecuting_eagerly() \u001B[38;5;129;01mor\u001B[39;00m ops\u001B[38;5;241m.\u001B[39minside_function():\n\u001B[1;32m    505\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mcolocate_with(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variant_tensor):\n\u001B[0;32m--> 506\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43miterator_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mOwnedIterator\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    507\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    508\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`tf.data.Dataset` only supports Python-style \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    509\u001B[0m                      \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miteration in eager mode or within tf.function.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/nyu2023summerml1-PUBzEmvh-py3.10/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:710\u001B[0m, in \u001B[0;36mOwnedIterator.__init__\u001B[0;34m(self, dataset, components, element_spec)\u001B[0m\n\u001B[1;32m    706\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m (components \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m element_spec \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    709\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnot be specified.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 710\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    712\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_next_call_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/nyu2023summerml1-PUBzEmvh-py3.10/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:749\u001B[0m, in \u001B[0;36mOwnedIterator._create_iterator\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    746\u001B[0m   \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(fulltype\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39margs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(\n\u001B[1;32m    747\u001B[0m       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_output_types)\n\u001B[1;32m    748\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator_resource\u001B[38;5;241m.\u001B[39mop\u001B[38;5;241m.\u001B[39mexperimental_set_type(fulltype)\n\u001B[0;32m--> 749\u001B[0m \u001B[43mgen_dataset_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mds_variant\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_iterator_resource\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/nyu2023summerml1-PUBzEmvh-py3.10/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3420\u001B[0m, in \u001B[0;36mmake_iterator\u001B[0;34m(dataset, iterator, name)\u001B[0m\n\u001B[1;32m   3418\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tld\u001B[38;5;241m.\u001B[39mis_eager:\n\u001B[1;32m   3419\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3420\u001B[0m     _result \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_FastPathExecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3421\u001B[0m \u001B[43m      \u001B[49m\u001B[43m_ctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mMakeIterator\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3422\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[1;32m   3423\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RfPO-DCg-rNf",
    "outputId": "bec0a531-75c0-4833-fdb6-5f06671072ac",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "start_time": "2023-06-27T16:33:52.962692Z"
    }
   },
   "source": [
    "print(len(history.history['loss']))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94hN6mpq-rNf"
   },
   "source": [
    "### Evaluate the performance of the model\n",
    "\n",
    "Using the `model.predict()` method we can evaluate the performance of our model on the testing dataset and compare it with the corresponding ground truth data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "89Ijildx-rNg"
   },
   "source": [
    "# prediction = model.predict(features)\n",
    "y_hat = model.predict(X_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UssrEc0S-rNg",
    "outputId": "10d5f062-2d13-49ef-bf4b-27fa66364808",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    }
   },
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_test.numpy(), Y_test.numpy(), label='Ground Truth')\n",
    "plt.scatter(X_test.numpy(), y_hat, marker='x', label='Prediction')\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mmFik-fu-rNg"
   },
   "source": [
    "loss_history = history.history['loss']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9rCND948-rNg",
    "outputId": "8a440b89-3d35-4b84-ff9a-04238d9df4c2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    }
   },
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.semilogy(loss_history)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqfQM65G-rNh"
   },
   "source": [
    "### Summary\n",
    "\n",
    "We can see that using Keras' Sequential API we can build a functional neural network using very few lines of code.\n",
    "```\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(1,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics='mse'\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=500, batch_size=32)\n",
    "```"
   ]
  }
 ]
}
